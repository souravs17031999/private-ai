{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated_learning_advanced_remote_tensoroperations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souravs17031999/private-ai/blob/master/federated_learning_advanced_remote_tensoroperations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdI9a5N6rp7D",
        "colab_type": "text"
      },
      "source": [
        "# PROJECT - XI\n",
        "## TO IMPLEMENT FEDERATED LEARNING ON TOY DATABASE \n",
        "\n",
        "### First we are going to train our model simply using toy dataset and then we will apply federated learning , thus using virtual workers to train our model on remote machines and simulate the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbnGQxgviorc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's first install pysyft and import our packages\n",
        "pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7tltPNpk4op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "dd2f5cd7-9b29-4264-a26e-a53bd273930e"
      },
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "import syft as sy\n",
        "from torch import nn, optim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0708 15:44:19.628088 140421954885504 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0708 15:44:19.650886 140421954885504 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jabM5-50lC-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(th)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2BBqwPAsF9v",
        "colab_type": "text"
      },
      "source": [
        "So, firstly , we will train the simple linear model on our local machine but his time \"using optimizers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JjxLART8VCp",
        "colab_type": "text"
      },
      "source": [
        "Let's create simple toy dataset including the data as well their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUPUAf9VlHLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = th.tensor([[1.,1],[0,1],[1,0],[0,0]], requires_grad=True)\n",
        "target = th.tensor([[1.],[1], [0], [0]], requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzk1QOIe8k2g",
        "colab_type": "text"
      },
      "source": [
        "Do you see we have written inside tensor like this - [1., 1] , [0, 1] and so the 'dot' represented in the first one will convert the whole tensor into float tensor implicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2M8xtQ8cis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b20d3438-9845-47e9-f139-202160e03fe2"
      },
      "source": [
        "print(data.dtype)\n",
        "print(target.dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev3ODoiw854-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model as it contains only two inputs and one output so we need only one linear layer \n",
        "model = nn.Linear(2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bUhN3W09PeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's use here SGD optimizer\n",
        "optimizer = optim.SGD(params=model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlE3hYEj9fdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's define a function for training loop\n",
        "def train(epochs):\n",
        "  avg_loss = []\n",
        "  for e in range(epochs): # iterating over the entire data \n",
        "    optimizer.zero_grad()   # clearing the gradients\n",
        "    pred = model(data)   # calcualting output of model \n",
        "    loss = ((pred - target)**2).sum()  # calculating loss (mistakes)\n",
        "    loss.backward()  # calculating gradients of error / loss function \n",
        "    optimizer.step()  # taking step in negative of gradient , gradient descent step\n",
        "    print(loss.data)  # printing training loss / mistakes \n",
        "    avg_loss.append(loss.data)\n",
        "  print(f\"avg loss : {sum(avg_loss)/len(avg_loss)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4XSU_gy97CZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "41647cad-01f7-4383-f7e7-75817628722a"
      },
      "source": [
        "train(20)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.0617)\n",
            "tensor(1.1503)\n",
            "tensor(0.7433)\n",
            "tensor(0.4915)\n",
            "tensor(0.3273)\n",
            "tensor(0.2193)\n",
            "tensor(0.1479)\n",
            "tensor(0.1004)\n",
            "tensor(0.0687)\n",
            "tensor(0.0473)\n",
            "tensor(0.0329)\n",
            "tensor(0.0230)\n",
            "tensor(0.0162)\n",
            "tensor(0.0115)\n",
            "tensor(0.0083)\n",
            "tensor(0.0060)\n",
            "tensor(0.0043)\n",
            "tensor(0.0032)\n",
            "tensor(0.0023)\n",
            "tensor(0.0017)\n",
            "avg loss : 0.27335864305496216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTlhaS2vNhEp",
        "colab_type": "text"
      },
      "source": [
        "Now, it's time for federated learning of this model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gFhh-XXWgkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = th.tensor([[1.,1],[0,1],[1,0],[0,0]], requires_grad=True)\n",
        "target = th.tensor([[1.],[1], [0], [0]], requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebH6AjkAn7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's start by creating and sending the datasets to the virtual workers we are going to create.\n",
        "bob = sy.VirtualWorker(hook, id = \"bob\")\n",
        "alice = sy.VirtualWorker(hook, id = \"alice\")\n",
        "\n",
        "data_bob = data[0:2].send(bob)\n",
        "target_bob = target[0:2].send(bob)\n",
        "data_alice = data[2:4].send(alice)\n",
        "target_alice = target[2:4].send(alice)\n",
        "datasets = [(data_bob, target_bob), (data_alice, target_alice)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H73GirFvOgVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fed(epochs):\n",
        "  model = nn.Linear(2,1)\n",
        "  optimizer = optim.SGD(params=model.parameters(), lr=0.1)\n",
        "  for e in range(epochs):\n",
        "    for inputs, labels in datasets:\n",
        "      model = model.send(inputs.location) # sending model to the location where tensors are located \n",
        "      optimizer.zero_grad()\n",
        "      pred = model(inputs)    \n",
        "      loss = ((pred - labels)**2).sum()  \n",
        "      loss.backward()   \n",
        "      optimizer.step()  \n",
        "      model =  model.get() # taking the model back with updates\n",
        "      \n",
        "      print(loss.get())   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VQOvaLdQNMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "694cb85e-0a3f-4939-8602-a522cefb1a9f"
      },
      "source": [
        "train_fed(20)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3.7706, requires_grad=True)\n",
            "tensor(0.0499, requires_grad=True)\n",
            "tensor(0.0577, requires_grad=True)\n",
            "tensor(0.0421, requires_grad=True)\n",
            "tensor(0.0280, requires_grad=True)\n",
            "tensor(0.0325, requires_grad=True)\n",
            "tensor(0.0204, requires_grad=True)\n",
            "tensor(0.0245, requires_grad=True)\n",
            "tensor(0.0152, requires_grad=True)\n",
            "tensor(0.0185, requires_grad=True)\n",
            "tensor(0.0114, requires_grad=True)\n",
            "tensor(0.0139, requires_grad=True)\n",
            "tensor(0.0085, requires_grad=True)\n",
            "tensor(0.0105, requires_grad=True)\n",
            "tensor(0.0063, requires_grad=True)\n",
            "tensor(0.0079, requires_grad=True)\n",
            "tensor(0.0047, requires_grad=True)\n",
            "tensor(0.0059, requires_grad=True)\n",
            "tensor(0.0035, requires_grad=True)\n",
            "tensor(0.0045, requires_grad=True)\n",
            "tensor(0.0026, requires_grad=True)\n",
            "tensor(0.0034, requires_grad=True)\n",
            "tensor(0.0020, requires_grad=True)\n",
            "tensor(0.0025, requires_grad=True)\n",
            "tensor(0.0015, requires_grad=True)\n",
            "tensor(0.0019, requires_grad=True)\n",
            "tensor(0.0011, requires_grad=True)\n",
            "tensor(0.0014, requires_grad=True)\n",
            "tensor(0.0008, requires_grad=True)\n",
            "tensor(0.0011, requires_grad=True)\n",
            "tensor(0.0006, requires_grad=True)\n",
            "tensor(0.0008, requires_grad=True)\n",
            "tensor(0.0005, requires_grad=True)\n",
            "tensor(0.0006, requires_grad=True)\n",
            "tensor(0.0003, requires_grad=True)\n",
            "tensor(0.0005, requires_grad=True)\n",
            "tensor(0.0003, requires_grad=True)\n",
            "tensor(0.0003, requires_grad=True)\n",
            "tensor(0.0002, requires_grad=True)\n",
            "tensor(0.0003, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mgYJ2zaefp9",
        "colab_type": "text"
      },
      "source": [
        "THIS FUNCTION HAS QUITE ISSUES IN CONTEXT TO DIFFERENTIAL PRIVACY MEASURES.\n",
        "Now, let's understand this , let's say we are in every iteration , we are sending model to one worker at a time and then getting back the model and then doing this for next worker and so on.\n",
        "Then this can be actually reversed engineer , so that we can find out who made the changes to the inputs we are feeding like in word embeddings etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL_pjkY0gIFy",
        "colab_type": "text"
      },
      "source": [
        "Therefore , we have to find ways to mitigate this problem and apply measures of privacy to prevent the above issue , one way to do this is to train the model with different workers at same time and then finally getting model back with averaged updates to the server.\n",
        "This will be quite difficult to find out who changed and influenced which weights because we have send the model to every worker at same time and then we are getting the model at the end of training with averaged updates from each of the workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt9L9_GHSHE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}