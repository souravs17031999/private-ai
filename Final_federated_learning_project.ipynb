{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_federated_learning_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souravs17031999/private-ai/blob/master/Final_federated_learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEGm3FrOVHZG",
        "colab_type": "text"
      },
      "source": [
        "# PROJECT - XII\n",
        "\n",
        "## OBJECTIVE : IMPLEMENTING FEDERATED LEARNING ON MNIST DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFN4gscSVWWe",
        "colab_type": "text"
      },
      "source": [
        "We have already implmented federated learning on toy dataset , now it's time for some real applications.\n",
        "Let's first install pysyft."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJwunSdESo9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6RDMH2cVkiw",
        "colab_type": "text"
      },
      "source": [
        "Start by importing all packages required for torch and pysyft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQ-Fu9aT7c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQuucJp_UxwY",
        "colab_type": "code",
        "outputId": "bc4e5d79-e132-42a6-86a3-20b75e7f4980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n",
        "# let's create two virtual workers who will hold the data while training the model locally\n",
        "bob = sy.VirtualWorker(hook, id = \"bob\")\n",
        "alice = sy.VirtualWorker(hook, id = \"alice\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0711 12:02:09.875396 139812741711744 hook.py:98] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u_R9O84jgH7",
        "colab_type": "text"
      },
      "source": [
        "It's time to load the data and apply our transforms !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er5u9ZG1ZvpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This arguments class is simply defining all the hyperparameters of the model we are going to train on\n",
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64 # batch size for training\n",
        "        self.test_batch_size = 1000 # bacth size for testing \n",
        "        self.epochs = 10 # no of epochs\n",
        "        self.lr = 0.01  # setting learning rate\n",
        "        self.momentum = 0.5 \n",
        "        self.no_cuda = False  \n",
        "        self.seed = 1\n",
        "        self.log_interval = 10\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed) # sets the random seed from pytorch random number generators\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\") # choose device based on what is available if cuda or not\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPiRgtw_0EKL",
        "colab_type": "text"
      },
      "source": [
        "We first load the data and transform the training Dataset into a Federated Dataset using the .federate method: it splits the dataset in two parts and send them to the workers alice and bob. This federated dataset is now given to a Federated DataLoader which will iterate over remote batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2JeGrfzVEW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using .federate method, we can distribute our data across all the workers for now , we have bob and alice\n",
        "federated_train_loader = sy.FederatedDataLoader( \n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ]))\n",
        "    .federate((bob, alice)), \n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ2XHwKnknsJ",
        "colab_type": "code",
        "outputId": "de067490-b44f-498b-fe2d-7cac7c73f30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om3aZ2yjIDhP",
        "colab_type": "text"
      },
      "source": [
        "Define the model for training and testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuny2RkGi4Zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.set_default_tensor_type(torch.cuda.FloatTensor) # setting the default dtyoe for all the tensors used here\n",
        "# define the model CNN architechture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IG_q5GQ21CV",
        "colab_type": "text"
      },
      "source": [
        "Below piece of code is our training loop which takes the inputs (data) and required outputs (target) and sends our model  to individual workers and then train them locally and then gets our model back so that we can predict the accuracy of images on our testing dataset.\n",
        "\n",
        "Let's first calc no of batches to be passed over 1 epoch : 60000/64 = 937\n",
        "so it sends some of the batches to bob and then next set of batches to alice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyzqIFxRmygL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # This sends our batches (half - 468) to first bob , and then to alice (next 469) - almost half on each of them\n",
        "        model.send(data.location) # sending the model to the right location \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # normal pytorch training\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        model.get() # one pass of training completed , now we get the model back \n",
        "        # printing each batch training progress \n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get() # <-- NEW: get the loss back\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAMtt150Xdww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normal pytorch testing loop \n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goEYBKPNIv3k",
        "colab_type": "text"
      },
      "source": [
        "Let's start actual training and testing loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e80Hqd33Xfw7",
        "colab_type": "code",
        "outputId": "ed286583-c3f2-4f1f-cb4e-85eaf0b620ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) \n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
        "    test(args, model, device, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.295970\n",
            "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.267599\n",
            "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.205497\n",
            "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.167159\n",
            "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.113135\n",
            "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.104657\n",
            "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.945493\n",
            "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.828618\n",
            "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.636697\n",
            "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.454292\n",
            "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.205155\n",
            "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 1.005845\n",
            "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.869017\n",
            "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.899048\n",
            "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.739474\n",
            "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.857247\n",
            "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.511609\n",
            "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.535861\n",
            "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.423060\n",
            "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.390930\n",
            "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.713601\n",
            "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.491273\n",
            "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.310572\n",
            "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.602596\n",
            "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.430848\n",
            "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.312132\n",
            "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.384986\n",
            "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.433828\n",
            "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.523123\n",
            "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.174579\n",
            "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.425527\n",
            "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.226813\n",
            "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.422522\n",
            "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.339706\n",
            "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.320272\n",
            "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.327703\n",
            "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.285000\n",
            "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.347190\n",
            "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.402485\n",
            "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.262023\n",
            "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.258042\n",
            "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.267939\n",
            "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.243946\n",
            "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.268981\n",
            "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.231549\n",
            "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.230875\n",
            "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.192416\n",
            "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.247099\n",
            "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.280132\n",
            "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.167989\n",
            "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.408889\n",
            "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.305867\n",
            "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.181627\n",
            "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.345720\n",
            "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.401033\n",
            "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.203847\n",
            "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.158687\n",
            "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.188594\n",
            "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.199814\n",
            "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.259604\n",
            "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.364576\n",
            "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.211647\n",
            "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.318508\n",
            "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.149336\n",
            "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.105087\n",
            "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.179589\n",
            "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.227670\n",
            "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.241868\n",
            "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.212688\n",
            "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.119687\n",
            "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.215460\n",
            "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.100331\n",
            "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.144880\n",
            "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.092493\n",
            "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.217694\n",
            "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.408697\n",
            "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.102224\n",
            "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.227672\n",
            "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.191036\n",
            "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.153197\n",
            "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.399090\n",
            "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.143720\n",
            "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.234842\n",
            "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.188849\n",
            "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.153855\n",
            "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.259114\n",
            "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.074155\n",
            "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.123922\n",
            "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.113665\n",
            "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.157964\n",
            "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.102677\n",
            "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.196737\n",
            "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.084427\n",
            "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.289282\n",
            "\n",
            "Test set: Average loss: 0.1628, Accuracy: 9515/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.247138\n",
            "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.254014\n",
            "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.234103\n",
            "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.170411\n",
            "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.153446\n",
            "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.172044\n",
            "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.093179\n",
            "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.140300\n",
            "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.170831\n",
            "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.241262\n",
            "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.185849\n",
            "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.093178\n",
            "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.374958\n",
            "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.133107\n",
            "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.177306\n",
            "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.129543\n",
            "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.145159\n",
            "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.071077\n",
            "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.156922\n",
            "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.095019\n",
            "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.195492\n",
            "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.157295\n",
            "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.315201\n",
            "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.417732\n",
            "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.100081\n",
            "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.119698\n",
            "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.081051\n",
            "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.180075\n",
            "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.331596\n",
            "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.167348\n",
            "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.141933\n",
            "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.143682\n",
            "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.156845\n",
            "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.116800\n",
            "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.047997\n",
            "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.101778\n",
            "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.244934\n",
            "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.045990\n",
            "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.090979\n",
            "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.070946\n",
            "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.126055\n",
            "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.114367\n",
            "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.056368\n",
            "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.061880\n",
            "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.137997\n",
            "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.050619\n",
            "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.069969\n",
            "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.174485\n",
            "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.131818\n",
            "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.101300\n",
            "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.152952\n",
            "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.150148\n",
            "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.053114\n",
            "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.194136\n",
            "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.111704\n",
            "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.101763\n",
            "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.114258\n",
            "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.119913\n",
            "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.099078\n",
            "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.257742\n",
            "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.164695\n",
            "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.116433\n",
            "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.159175\n",
            "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.054346\n",
            "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.082250\n",
            "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.117730\n",
            "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.037644\n",
            "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.060700\n",
            "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.115378\n",
            "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.053151\n",
            "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.363471\n",
            "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.095963\n",
            "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.165914\n",
            "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.099518\n",
            "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.108763\n",
            "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.217186\n",
            "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.213339\n",
            "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.095458\n",
            "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.187710\n",
            "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.098105\n",
            "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.135343\n",
            "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.067969\n",
            "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.077561\n",
            "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.103163\n",
            "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.105532\n",
            "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.084379\n",
            "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.238382\n",
            "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.093421\n",
            "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.087459\n",
            "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.234088\n",
            "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.129877\n",
            "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.083564\n",
            "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.095942\n",
            "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.068586\n",
            "\n",
            "Test set: Average loss: 0.0926, Accuracy: 9712/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.051091\n",
            "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.079059\n",
            "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.090380\n",
            "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.041348\n",
            "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.137595\n",
            "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.067109\n",
            "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.035271\n",
            "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.280271\n",
            "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.042068\n",
            "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.182280\n",
            "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.070993\n",
            "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.121865\n",
            "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.116487\n",
            "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.125217\n",
            "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.139973\n",
            "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.049511\n",
            "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.108510\n",
            "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.076529\n",
            "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.121687\n",
            "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.137617\n",
            "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.223965\n",
            "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.084320\n",
            "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.116443\n",
            "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.036330\n",
            "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.055185\n",
            "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.095449\n",
            "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.073443\n",
            "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.079096\n",
            "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.130052\n",
            "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.048217\n",
            "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.100788\n",
            "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.185564\n",
            "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.059519\n",
            "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.040338\n",
            "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.042022\n",
            "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.047968\n",
            "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.097617\n",
            "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.088838\n",
            "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.260882\n",
            "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.093192\n",
            "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.061084\n",
            "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.074673\n",
            "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.042397\n",
            "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.102489\n",
            "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.107540\n",
            "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.074592\n",
            "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.061640\n",
            "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.048553\n",
            "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.071893\n",
            "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.075801\n",
            "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.043648\n",
            "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.063633\n",
            "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.052429\n",
            "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.102605\n",
            "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.045829\n",
            "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.050378\n",
            "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.097045\n",
            "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.108995\n",
            "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.071582\n",
            "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.099767\n",
            "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.071131\n",
            "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.087118\n",
            "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.026666\n",
            "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.058590\n",
            "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.070501\n",
            "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.091375\n",
            "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.020416\n",
            "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.028662\n",
            "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.126763\n",
            "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.091299\n",
            "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.045547\n",
            "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.130833\n",
            "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.084923\n",
            "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.020834\n",
            "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.087880\n",
            "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.026355\n",
            "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.105554\n",
            "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.031078\n",
            "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.032439\n",
            "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.123194\n",
            "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.016598\n",
            "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.112322\n",
            "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.065108\n",
            "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.083037\n",
            "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.094475\n",
            "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.087521\n",
            "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.013469\n",
            "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.059953\n",
            "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.062314\n",
            "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.072292\n",
            "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.033173\n",
            "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.025270\n",
            "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.120961\n",
            "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.085058\n",
            "\n",
            "Test set: Average loss: 0.0657, Accuracy: 9788/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.092560\n",
            "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.014123\n",
            "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.063045\n",
            "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.216293\n",
            "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.056870\n",
            "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.095152\n",
            "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.089549\n",
            "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.045873\n",
            "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.089255\n",
            "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.109575\n",
            "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.054700\n",
            "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.018641\n",
            "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.012546\n",
            "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.046785\n",
            "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.060228\n",
            "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.052980\n",
            "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.028254\n",
            "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.090408\n",
            "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.056277\n",
            "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.029625\n",
            "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.139324\n",
            "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.051117\n",
            "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.025397\n",
            "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.034252\n",
            "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.035270\n",
            "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.055895\n",
            "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.158260\n",
            "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.146362\n",
            "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.044316\n",
            "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.020250\n",
            "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.010685\n",
            "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.115349\n",
            "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.046215\n",
            "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.050512\n",
            "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.081105\n",
            "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.135378\n",
            "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.090053\n",
            "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.074011\n",
            "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.116072\n",
            "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.036207\n",
            "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.220258\n",
            "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.130419\n",
            "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.104396\n",
            "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.032018\n",
            "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.088079\n",
            "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.050257\n",
            "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.029579\n",
            "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.053545\n",
            "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.015373\n",
            "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.096609\n",
            "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.048626\n",
            "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.052488\n",
            "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.026867\n",
            "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.024912\n",
            "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.073224\n",
            "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.040388\n",
            "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.102564\n",
            "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.034855\n",
            "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.157865\n",
            "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.057889\n",
            "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.084496\n",
            "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.026786\n",
            "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.033178\n",
            "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.105008\n",
            "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.096689\n",
            "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.105095\n",
            "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.015212\n",
            "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.060343\n",
            "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.148402\n",
            "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.042544\n",
            "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.046629\n",
            "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.079088\n",
            "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.024631\n",
            "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.094001\n",
            "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.030109\n",
            "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.072422\n",
            "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.109040\n",
            "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.014641\n",
            "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.230877\n",
            "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.050243\n",
            "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.068309\n",
            "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.071980\n",
            "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.069364\n",
            "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.065950\n",
            "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.011511\n",
            "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.125911\n",
            "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.025139\n",
            "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.105099\n",
            "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.059834\n",
            "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.143020\n",
            "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.134530\n",
            "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.065349\n",
            "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.337692\n",
            "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.166753\n",
            "\n",
            "Test set: Average loss: 0.0647, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.021451\n",
            "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.024919\n",
            "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.102265\n",
            "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.028806\n",
            "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.056523\n",
            "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.089888\n",
            "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.047205\n",
            "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.057749\n",
            "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.053783\n",
            "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.054801\n",
            "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.027946\n",
            "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.033654\n",
            "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.025624\n",
            "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.016702\n",
            "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.193638\n",
            "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.030310\n",
            "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.140694\n",
            "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.073674\n",
            "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.042346\n",
            "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.035294\n",
            "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.004633\n",
            "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.044437\n",
            "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.013069\n",
            "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.007396\n",
            "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.017417\n",
            "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.050029\n",
            "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.019051\n",
            "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.027387\n",
            "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.056131\n",
            "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.034112\n",
            "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.041192\n",
            "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.039892\n",
            "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.056918\n",
            "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.009373\n",
            "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.012542\n",
            "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.019554\n",
            "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.035167\n",
            "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.024694\n",
            "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.081526\n",
            "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.054104\n",
            "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.051163\n",
            "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.102188\n",
            "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.036057\n",
            "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.051428\n",
            "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.055637\n",
            "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.051154\n",
            "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.034308\n",
            "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.082510\n",
            "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.009314\n",
            "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.037826\n",
            "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.025922\n",
            "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.033834\n",
            "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.032886\n",
            "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.106942\n",
            "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.034216\n",
            "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.085582\n",
            "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.202496\n",
            "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.012849\n",
            "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.058313\n",
            "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.029710\n",
            "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.045598\n",
            "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.042607\n",
            "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.012171\n",
            "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.033719\n",
            "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.009728\n",
            "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.032597\n",
            "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.100492\n",
            "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.010937\n",
            "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.011980\n",
            "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.122144\n",
            "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.127672\n",
            "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.017005\n",
            "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.003834\n",
            "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.008195\n",
            "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.014431\n",
            "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.009667\n",
            "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.094719\n",
            "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.094724\n",
            "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.045585\n",
            "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.030973\n",
            "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.037966\n",
            "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.096966\n",
            "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.081910\n",
            "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.063220\n",
            "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.065162\n",
            "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.047844\n",
            "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.022354\n",
            "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.034232\n",
            "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.037632\n",
            "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.013641\n",
            "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.025479\n",
            "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.030533\n",
            "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.024188\n",
            "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.041910\n",
            "\n",
            "Test set: Average loss: 0.0578, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.033542\n",
            "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.099611\n",
            "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.026894\n",
            "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.043879\n",
            "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.056142\n",
            "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.049477\n",
            "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.103860\n",
            "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.027214\n",
            "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.025096\n",
            "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.023874\n",
            "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.044333\n",
            "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.115876\n",
            "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.057677\n",
            "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.071447\n",
            "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.017041\n",
            "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.043985\n",
            "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.034478\n",
            "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.137572\n",
            "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.069018\n",
            "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.031747\n",
            "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.048185\n",
            "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.025609\n",
            "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.023071\n",
            "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.054139\n",
            "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.041872\n",
            "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.030542\n",
            "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.029030\n",
            "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.027914\n",
            "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.018307\n",
            "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.119155\n",
            "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.028922\n",
            "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.059737\n",
            "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.028520\n",
            "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.063922\n",
            "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.017616\n",
            "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.054988\n",
            "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.010584\n",
            "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.127143\n",
            "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.068198\n",
            "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.056928\n",
            "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.098504\n",
            "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.020034\n",
            "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.019835\n",
            "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.032554\n",
            "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.068302\n",
            "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.168960\n",
            "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.009078\n",
            "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.035041\n",
            "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.007779\n",
            "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.086032\n",
            "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.006280\n",
            "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.018547\n",
            "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.072929\n",
            "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.052018\n",
            "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.012485\n",
            "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.011716\n",
            "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.241130\n",
            "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.089725\n",
            "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.015337\n",
            "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.165133\n",
            "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.036321\n",
            "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.010768\n",
            "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.066283\n",
            "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.019390\n",
            "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.051853\n",
            "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.082836\n",
            "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.090130\n",
            "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.022022\n",
            "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.004189\n",
            "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.017867\n",
            "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.064569\n",
            "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.043357\n",
            "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.140581\n",
            "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.029671\n",
            "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.036464\n",
            "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.083316\n",
            "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.140288\n",
            "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.122274\n",
            "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.032793\n",
            "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.076420\n",
            "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.013713\n",
            "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.026174\n",
            "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.016894\n",
            "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.010057\n",
            "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.037098\n",
            "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.005035\n",
            "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.098063\n",
            "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.157421\n",
            "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.028189\n",
            "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.009614\n",
            "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.072937\n",
            "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.021928\n",
            "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.030324\n",
            "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.010085\n",
            "\n",
            "Test set: Average loss: 0.0455, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.039144\n",
            "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.023539\n",
            "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.040591\n",
            "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.028864\n",
            "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.074492\n",
            "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.037874\n",
            "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.007855\n",
            "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.083625\n",
            "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.020570\n",
            "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.026699\n",
            "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.137690\n",
            "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.009490\n",
            "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.040079\n",
            "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.046045\n",
            "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.101093\n",
            "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.035304\n",
            "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.015307\n",
            "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.056456\n",
            "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.076519\n",
            "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.098481\n",
            "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.016735\n",
            "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.148734\n",
            "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.002763\n",
            "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.065393\n",
            "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.025367\n",
            "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.021357\n",
            "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.063145\n",
            "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.094405\n",
            "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.049924\n",
            "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.017882\n",
            "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.023012\n",
            "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.031982\n",
            "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.007803\n",
            "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.041331\n",
            "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.188628\n",
            "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.057881\n",
            "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.042923\n",
            "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.040246\n",
            "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.033829\n",
            "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.164279\n",
            "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.007794\n",
            "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.033241\n",
            "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.018403\n",
            "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.037024\n",
            "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.028314\n",
            "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.029199\n",
            "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.021607\n",
            "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.011347\n",
            "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.106486\n",
            "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.040365\n",
            "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.096263\n",
            "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.031576\n",
            "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.064461\n",
            "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.077900\n",
            "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.005397\n",
            "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.049509\n",
            "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.024695\n",
            "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.007570\n",
            "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.018394\n",
            "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.050025\n",
            "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.102181\n",
            "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.007271\n",
            "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.034892\n",
            "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.062255\n",
            "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.056334\n",
            "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.030906\n",
            "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.017727\n",
            "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.027384\n",
            "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.064684\n",
            "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.037760\n",
            "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.085424\n",
            "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.026761\n",
            "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.063348\n",
            "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.023341\n",
            "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.090561\n",
            "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.035247\n",
            "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.021961\n",
            "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.029640\n",
            "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.352478\n",
            "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.078074\n",
            "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.012748\n",
            "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.039325\n",
            "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.003001\n",
            "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.006152\n",
            "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.098751\n",
            "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.057402\n",
            "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.032254\n",
            "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.030341\n",
            "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.043283\n",
            "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.026728\n",
            "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.017605\n",
            "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.049861\n",
            "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.057398\n",
            "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.067718\n",
            "\n",
            "Test set: Average loss: 0.0418, Accuracy: 9871/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.053585\n",
            "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.092810\n",
            "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.116525\n",
            "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.011548\n",
            "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.040197\n",
            "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.014837\n",
            "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.112198\n",
            "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.038595\n",
            "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.050111\n",
            "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.005515\n",
            "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.021664\n",
            "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.135359\n",
            "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.046731\n",
            "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.028538\n",
            "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.019896\n",
            "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.008850\n",
            "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.023613\n",
            "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.008601\n",
            "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.065298\n",
            "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.088422\n",
            "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.022704\n",
            "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.024063\n",
            "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.014324\n",
            "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.010942\n",
            "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.024096\n",
            "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.109427\n",
            "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.090669\n",
            "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.019040\n",
            "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.006742\n",
            "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.009802\n",
            "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.124050\n",
            "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.008407\n",
            "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.050155\n",
            "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.058563\n",
            "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.061033\n",
            "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.046917\n",
            "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.043053\n",
            "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.056014\n",
            "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.045457\n",
            "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.026841\n",
            "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.020815\n",
            "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.017812\n",
            "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.008066\n",
            "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.033583\n",
            "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.072364\n",
            "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.085067\n",
            "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.107008\n",
            "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.004210\n",
            "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.024896\n",
            "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.024898\n",
            "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.016486\n",
            "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.027163\n",
            "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.018815\n",
            "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.025291\n",
            "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.101915\n",
            "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.016872\n",
            "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.056927\n",
            "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.067036\n",
            "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.059275\n",
            "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.090839\n",
            "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.057407\n",
            "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.058943\n",
            "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.053843\n",
            "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.080529\n",
            "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.020823\n",
            "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.010964\n",
            "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.050956\n",
            "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.031078\n",
            "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.037485\n",
            "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.027638\n",
            "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.032243\n",
            "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.056118\n",
            "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.019754\n",
            "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.010634\n",
            "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.173398\n",
            "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.015890\n",
            "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.061209\n",
            "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.021444\n",
            "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.007830\n",
            "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.003301\n",
            "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.094296\n",
            "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.026672\n",
            "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.047444\n",
            "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.008022\n",
            "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.050508\n",
            "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.053140\n",
            "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.013925\n",
            "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.097443\n",
            "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.020202\n",
            "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.014428\n",
            "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.009097\n",
            "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.006595\n",
            "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.030740\n",
            "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.024301\n",
            "\n",
            "Test set: Average loss: 0.0444, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.011676\n",
            "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.084051\n",
            "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.042922\n",
            "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.013073\n",
            "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.023390\n",
            "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.043010\n",
            "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.033846\n",
            "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.011302\n",
            "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.015201\n",
            "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.091891\n",
            "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.016993\n",
            "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.006464\n",
            "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.300538\n",
            "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.057519\n",
            "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.118159\n",
            "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.007880\n",
            "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.008238\n",
            "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.100480\n",
            "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.042164\n",
            "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.047927\n",
            "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.101466\n",
            "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.003543\n",
            "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.004488\n",
            "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.010268\n",
            "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.009874\n",
            "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.044770\n",
            "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.009006\n",
            "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.081993\n",
            "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.024094\n",
            "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.028484\n",
            "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.050120\n",
            "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.005743\n",
            "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.034773\n",
            "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.029644\n",
            "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.032066\n",
            "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.028403\n",
            "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.086993\n",
            "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.038043\n",
            "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.010003\n",
            "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.077716\n",
            "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.014014\n",
            "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.050371\n",
            "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.018833\n",
            "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.011644\n",
            "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.032854\n",
            "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.028915\n",
            "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.025541\n",
            "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.007217\n",
            "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.031540\n",
            "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.072926\n",
            "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.037141\n",
            "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.043765\n",
            "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.104717\n",
            "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.014544\n",
            "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.049799\n",
            "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.002549\n",
            "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.031622\n",
            "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.009557\n",
            "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.012664\n",
            "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.032828\n",
            "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.005836\n",
            "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.054666\n",
            "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.013809\n",
            "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.010888\n",
            "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.037440\n",
            "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.026387\n",
            "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.038224\n",
            "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.069546\n",
            "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.023721\n",
            "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.013051\n",
            "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.048281\n",
            "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.080138\n",
            "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.022208\n",
            "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.035562\n",
            "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.025848\n",
            "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.004855\n",
            "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.081223\n",
            "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.060916\n",
            "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.123305\n",
            "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.088721\n",
            "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.037887\n",
            "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.010590\n",
            "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.003052\n",
            "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.071474\n",
            "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.077172\n",
            "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.063621\n",
            "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.011151\n",
            "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.055571\n",
            "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.039344\n",
            "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.008914\n",
            "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.004351\n",
            "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.027532\n",
            "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.073254\n",
            "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.022165\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.014066\n",
            "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.019091\n",
            "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.009863\n",
            "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.004777\n",
            "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.009028\n",
            "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.025215\n",
            "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.153971\n",
            "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.024469\n",
            "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.022491\n",
            "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.021150\n",
            "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.026684\n",
            "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.042470\n",
            "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.146989\n",
            "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.030984\n",
            "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.015725\n",
            "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.050126\n",
            "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.022888\n",
            "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.010529\n",
            "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.010146\n",
            "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.006982\n",
            "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.003768\n",
            "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.003915\n",
            "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.027319\n",
            "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.054081\n",
            "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.100273\n",
            "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.014101\n",
            "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.002044\n",
            "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.016205\n",
            "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.011073\n",
            "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.031557\n",
            "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.010273\n",
            "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.010412\n",
            "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.003535\n",
            "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.026697\n",
            "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.014435\n",
            "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.008317\n",
            "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.006036\n",
            "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.024859\n",
            "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.034908\n",
            "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.042078\n",
            "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.035114\n",
            "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.031135\n",
            "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.057296\n",
            "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.038621\n",
            "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.026147\n",
            "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.008579\n",
            "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.027437\n",
            "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.020613\n",
            "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.066328\n",
            "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.046669\n",
            "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.022500\n",
            "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.011413\n",
            "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.047533\n",
            "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.055602\n",
            "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.008883\n",
            "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.041137\n",
            "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.037511\n",
            "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.150381\n",
            "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.046936\n",
            "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.006391\n",
            "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.007107\n",
            "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.049135\n",
            "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.006802\n",
            "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.026974\n",
            "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.020990\n",
            "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.105726\n",
            "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.039489\n",
            "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.154099\n",
            "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.065330\n",
            "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.064259\n",
            "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.076974\n",
            "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.031056\n",
            "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.014342\n",
            "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.018572\n",
            "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.057241\n",
            "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.035983\n",
            "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.030195\n",
            "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.034348\n",
            "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.006866\n",
            "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.042576\n",
            "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.072048\n",
            "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.002612\n",
            "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.029833\n",
            "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.077033\n",
            "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.077397\n",
            "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.012432\n",
            "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.020539\n",
            "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.012841\n",
            "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.010641\n",
            "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.073584\n",
            "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.014440\n",
            "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.038011\n",
            "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.069627\n",
            "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.080690\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 9888/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onxpvJvCI2Jn",
        "colab_type": "text"
      },
      "source": [
        "Hurray ! We completed federated learning technique for MNIST database succesfully with almost 99% accuracy."
      ]
    }
  ]
}